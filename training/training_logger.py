"""Training logger for saving metrics to file for analysis and graphing."""

import json
import os
from datetime import datetime
from typing import Dict, Any, List


class TrainingLogger:
    """Logs training metrics to JSON file incrementally."""
    
    def __init__(self, log_file: str = "training_log.json"):
        """Initialize logger.
        
        Args:
            log_file: Path to JSON log file
        """
        self.log_file = log_file
        self.session_start = datetime.now().isoformat()
        
        # Initialize or load existing log
        if os.path.exists(log_file):
            with open(log_file, 'r') as f:
                self.data = json.load(f)
        else:
            self.data = {
                "sessions": []
            }
        
        # Start new session
        self.current_session = {
            "start_time": self.session_start,
            "steps": []
        }
        self.data["sessions"].append(self.current_session)
        self._save()
    
    def log_step(
        self,
        step: int,
        stage_id: int,
        model_type: str,  # "discriminator" or "generator"
        problem_id: str,
        
        # Reward metrics
        disc_reward: float,
        gen_reward: float,
        
        # Test execution metrics
        num_tests_generated: int,
        num_tests_valid: int,
        num_tests_passed_gen: int,
        num_tests_total_gen: int,
        num_tests_passed_val: int,
        num_tests_total_val: int,
        
        # Combined metrics (with baseline)
        num_tests_combined: int,
        num_tests_passed_combined: int,
        
        # Loss
        loss: float = None,
        
        # Code quality metrics
        code_length: int = 0,
        has_syntax_error: bool = False,
        
        # Test quality samples
        test_samples: List[str] = None
    ):
        """Log a training step.
        
        Args:
            step: Global step number
            stage_id: Reasoning stage (1-5)
            model_type: Which model is being trained
            problem_id: Problem identifier
            disc_reward: Discriminator reward
            gen_reward: Generator reward
            num_tests_generated: Number of tests generated by discriminator
            num_tests_valid: Number of tests that passed validation
            num_tests_passed_gen: Tests passed by generator code
            num_tests_total_gen: Total tests run on generator
            num_tests_passed_val: Tests passed by validation (ground truth)
            num_tests_total_val: Total tests run on validation
            num_tests_combined: Total tests including baseline
            num_tests_passed_combined: Tests passed including baseline
            loss: Training loss (if available)
            code_length: Length of generated code
            has_syntax_error: Whether code has syntax errors
            test_samples: Sample test cases for inspection
        """
        step_data = {
            "step": step,
            "timestamp": datetime.now().isoformat(),
            "stage_id": stage_id,
            "model_type": model_type,
            "problem_id": problem_id,
            
            # Rewards
            "disc_reward": disc_reward,
            "gen_reward": gen_reward,
            
            # Test metrics
            "num_tests_generated": num_tests_generated,
            "num_tests_valid": num_tests_valid,
            "num_tests_passed_gen": num_tests_passed_gen,
            "num_tests_total_gen": num_tests_total_gen,
            "num_tests_passed_val": num_tests_passed_val,
            "num_tests_total_val": num_tests_total_val,
            "num_tests_combined": num_tests_combined,
            "num_tests_passed_combined": num_tests_passed_combined,
            
            # Derived metrics
            "gen_pass_rate": num_tests_passed_gen / num_tests_total_gen if num_tests_total_gen > 0 else 0.0,
            "val_pass_rate": num_tests_passed_val / num_tests_total_val if num_tests_total_val > 0 else 0.0,
            "combined_pass_rate": num_tests_passed_combined / num_tests_combined if num_tests_combined > 0 else 0.0,
            "test_validity_rate": num_tests_valid / num_tests_generated if num_tests_generated > 0 else 0.0,
            
            # Code quality
            "code_length": code_length,
            "has_syntax_error": has_syntax_error,
            
            # Loss
            "loss": loss
        }
        
        # Add test samples if provided (for debugging/inspection)
        if test_samples:
            step_data["test_samples"] = test_samples[:5]  # Keep first 5
        
        self.current_session["steps"].append(step_data)
        self._save()
    
    def log_epoch_summary(
        self,
        epoch: int,
        stage_id: int,
        model_type: str,
        metrics: Dict[str, Any]
    ):
        """Log summary metrics for an epoch.
        
        Args:
            epoch: Epoch number
            stage_id: Reasoning stage
            model_type: Which model
            metrics: Dictionary of summary metrics
        """
        if "epoch_summaries" not in self.current_session:
            self.current_session["epoch_summaries"] = []
        
        summary = {
            "epoch": epoch,
            "stage_id": stage_id,
            "model_type": model_type,
            "timestamp": datetime.now().isoformat(),
            **metrics
        }
        
        self.current_session["epoch_summaries"].append(summary)
        self._save()
    
    def end_session(self):
        """Mark session as complete."""
        self.current_session["end_time"] = datetime.now().isoformat()
        self._save()
    
    def _save(self):
        """Save current data to file."""
        with open(self.log_file, 'w') as f:
            json.dump(self.data, f, indent=2)
    
    def get_summary_stats(self) -> Dict[str, Any]:
        """Get summary statistics for current session.
        
        Returns:
            Dictionary with summary stats
        """
        steps = self.current_session["steps"]
        if not steps:
            return {}
        
        # Compute averages
        disc_steps = [s for s in steps if s["model_type"] == "discriminator"]
        gen_steps = [s for s in steps if s["model_type"] == "generator"]
        
        return {
            "total_steps": len(steps),
            "disc_steps": len(disc_steps),
            "gen_steps": len(gen_steps),
            "avg_disc_reward": sum(s["disc_reward"] for s in disc_steps) / len(disc_steps) if disc_steps else 0,
            "avg_gen_reward": sum(s["gen_reward"] for s in gen_steps) / len(gen_steps) if gen_steps else 0,
            "avg_gen_pass_rate": sum(s["gen_pass_rate"] for s in steps) / len(steps),
            "avg_val_pass_rate": sum(s["val_pass_rate"] for s in steps) / len(steps),
            "avg_test_validity": sum(s["test_validity_rate"] for s in steps) / len(steps),
            "syntax_error_rate": sum(1 for s in steps if s["has_syntax_error"]) / len(steps)
        }
