{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2FZkfKYVwNf",
        "outputId": "8f4d3ff1-a868-483e-fb68-0076663700c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chained-adversarial-reasoning'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 160 (delta 63), reused 132 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (160/160), 123.71 KiB | 5.62 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NitinSudarsanam/chained-adversarial-reasoning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd chained-adversarial-reasoning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLOQNLfzWSt7",
        "outputId": "7f8641c7-00e0-40f6-cb8b-3567893ce829"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chained-adversarial-reasoning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers accelerate tqdm -q"
      ],
      "metadata": {
        "id": "sJxoeJ_wWbvI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python show_model_output.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAlO438XllIZ",
        "outputId": "9c341fba-702b-47ac-c97a-a4f9b9fa88a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model (takes ~30 seconds)...\n",
            "Loading generator model: Qwen/Qwen2.5-Coder-3B-Instruct\n",
            "tokenizer_config.json: 7.30kB [00:00, 33.1MB/s]\n",
            "vocab.json: 2.78MB [00:00, 93.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 139MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 185MB/s]\n",
            "config.json: 100% 661/661 [00:00<00:00, 6.70MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-12-01 02:05:47.311795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764554747.329867     298 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764554747.334706     298 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764554747.347578     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764554747.347600     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764554747.347604     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764554747.347610     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-01 02:05:47.351543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors.index.json: 35.6kB [00:00, 136MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/1.21G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 1.07M/1.21G [00:01<27:17, 741kB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 8.19M/4.96G [00:01<17:10, 4.80MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   1% 8.75M/1.21G [00:02<05:33, 3.61MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 27.2M/4.96G [00:03<09:00, 9.13MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 67.8M/4.96G [00:05<06:19, 12.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.2M/4.96G [00:06<04:03, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.96G [00:06<02:51, 28.1MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 203M/4.96G [00:08<02:32, 31.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 270M/4.96G [00:08<01:32, 50.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 337M/4.96G [00:09<01:03, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 404M/4.96G [00:09<00:46, 98.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 471M/4.96G [00:09<00:38, 118MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 538M/4.96G [00:10<00:31, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 605M/4.96G [00:10<00:28, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 672M/4.96G [00:10<00:25, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 739M/4.96G [00:11<00:26, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 806M/4.96G [00:11<00:26, 158MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   6% 75.7M/1.21G [00:11<02:44, 6.92MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 873M/4.96G [00:12<00:24, 164MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  12% 142M/1.21G [00:14<01:35, 11.2MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 940M/4.96G [00:14<01:06, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.96G [00:15<00:50, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.96G [00:15<00:37, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.96G [00:15<00:31, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.96G [00:15<00:24, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.96G [00:15<00:20, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.96G [00:16<00:18, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.96G [00:16<00:16, 209MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  17% 209M/1.21G [00:17<01:05, 15.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  23% 276M/1.21G [00:17<00:38, 24.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  28% 343M/1.21G [00:17<00:24, 36.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  34% 410M/1.21G [00:17<00:15, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 477M/1.21G [00:18<00:10, 69.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.96G [00:18<00:37, 92.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.96G [00:18<00:32, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.96G [00:18<00:25, 134MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  45% 544M/1.21G [00:20<00:15, 44.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.96G [00:20<00:52, 62.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  50% 611M/1.21G [00:21<00:10, 59.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  56% 678M/1.21G [00:21<00:06, 78.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  61% 745M/1.21G [00:21<00:04, 95.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.96G [00:21<00:48, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.96G [00:21<00:36, 85.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  67% 812M/1.21G [00:22<00:03, 106MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.96G [00:22<00:30, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.96G [00:22<00:24, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.96G [00:24<00:47, 63.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  72% 879M/1.21G [00:24<00:06, 52.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.96G [00:25<00:34, 83.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  78% 946M/1.21G [00:25<00:04, 66.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.96G [00:25<00:28, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.96G [00:25<00:21, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.96G [00:25<00:18, 149MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 1.01G/1.21G [00:26<00:02, 69.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  89% 1.08G/1.21G [00:26<00:01, 81.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  94% 1.15G/1.21G [00:26<00:00, 104MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.96G [00:27<00:25, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.96G [00:27<00:19, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.96G [00:27<00:16, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.96G [00:27<00:15, 156MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 1.21G/1.21G [00:27<00:00, 43.4MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.96G [00:28<00:14, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.96G [00:28<00:12, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.96G [00:28<00:11, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.96G [00:29<00:11, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.96G [00:29<00:11, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.91G/4.96G [00:31<00:23, 87.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.96G [00:31<00:16, 117MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.96G [00:31<00:14, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.96G [00:32<00:17, 106MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.96G [00:33<00:18, 95.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.96G [00:33<00:14, 123MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.96G [00:33<00:11, 148MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.35G/4.96G [00:33<00:09, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.96G [00:34<00:08, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.96G [00:34<00:08, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.55G/4.96G [00:34<00:06, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.96G [00:34<00:05, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.96G [00:35<00:04, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.75G/4.96G [00:35<00:04, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.96G [00:35<00:04, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.96G [00:35<00:03, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.96G [00:36<00:03, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.96G [00:36<00:03, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.96G [00:36<00:03, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.96G [00:36<00:03, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.96G [00:37<00:02, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.96G [00:39<00:08, 83.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.96G [00:39<00:05, 112MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.96G [00:39<00:03, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.49G/4.96G [00:39<00:02, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.96G [00:39<00:02, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.96G [00:40<00:01, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.69G/4.96G [00:40<00:01, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.96G [00:40<00:00, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.96G [00:40<00:00, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.89G/4.96G [00:41<00:00, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.96G/4.96G [00:43<00:00, 114MB/s] \n",
            "Fetching 2 files: 100% 2/2 [00:43<00:00, 21.81s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:22<00:00, 11.36s/it]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 2.52MB/s]\n",
            "\n",
            "================================================================================\n",
            "PROBLEM: string_compression_with_frequency\n",
            "================================================================================\n",
            "\n",
            "Description:\n",
            "Implement a string compression algorithm that:\n",
            "1. Groups consecutive identical characters\n",
            "2. Replaces groups of 3+ chars with: char + count\n",
            "3. Keeps groups of 1-2 chars as-is\n",
            "4. Returns the shorter of original or compressed string\n",
            "Example: \"aaabbc\" -> \"a3bbc\" (7 chars -> 5 chars)\n",
            "Example: \"abc\" -> \"abc\" (no compression needed)\n",
            "\n",
            "Required Signature:\n",
            "def compress_string(s: str) -> str:\n",
            "\n",
            "================================================================================\n",
            "PROMPT SENT TO MODEL:\n",
            "================================================================================\n",
            "You are a Python coding assistant. Write a complete, working Python function.\n",
            "\n",
            "IMPORTANT: \n",
            "- Keep the EXACT function signature provided\n",
            "- Write actual working code, not comments or placeholders\n",
            "- Return the result\n",
            "\n",
            "Function to implement:\n",
            "def compress_string(s: str) -> str:\n",
            "\n",
            "Problem description:\n",
            "Implement a string compression algorithm that:\n",
            "1. Groups consecutive identical characters\n",
            "2. Replaces groups of 3+ chars with: char + count\n",
            "3. Keeps groups of 1-2 chars as-is\n",
            "4. Returns the shorter of original or compressed string\n",
            "Example: \"aaabbc\" -> \"a3bbc\" (7 chars -> 5 chars)\n",
            "Example: \"abc\" -> \"abc\" (no compression needed)\n",
            "\n",
            "Write the complete function below (with actual code, not 'pass'):\n",
            "\n",
            "```python\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Generating code (takes ~30-60 seconds on CPU)...\n",
            "\n",
            "================================================================================\n",
            "MODEL OUTPUT (after cleaning):\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Test Results: 0/4 passed\n",
            "Errors: ['Traceback (most recent call last):', '  File \"<string>\", line 4, in <module>', \"NameError: name 'compress_string' is not defined\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_unified_training.py --device cuda --n-steps 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJsz2pB5aEdh",
        "outputId": "fc07b9fa-46ad-4ca5-e038-5bb16ca38d1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared GPU cache\n",
            "================================================================================\n",
            "UNIFIED MODEL TRAINING\n",
            "================================================================================\n",
            "\n",
            "Configuration:\n",
            "  Model: Qwen/Qwen2.5-Coder-3B-Instruct\n",
            "  Device: cuda\n",
            "  Problems: data/function_problems.json\n",
            "  Steps per stage: 2\n",
            "  Learning rate: 1e-05\n",
            "\n",
            "GPU Memory:\n",
            "  Total: 14.74 GiB\n",
            "  Allocated: 0.00 GiB\n",
            "  Reserved: 0.00 GiB\n",
            "  Free: 14.74 GiB\n",
            "\n",
            "Loading problems...\n",
            "Loaded 5 problems\n",
            "\n",
            "Initializing unified model...\n",
            "Loading unified model: Qwen/Qwen2.5-Coder-3B-Instruct\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-12-01 02:07:49.052972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764554869.071321     910 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764554869.076421     910 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764554869.089694     910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764554869.089715     910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764554869.089719     910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764554869.089726     910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-01 02:07:49.093732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading checkpoint shards: 100% 2/2 [00:26<00:00, 13.41s/it]\n",
            "\n",
            "\n",
            "============================================================\n",
            "STARTING FULL PIPELINE TRAINING (UNIFIED MODEL)\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "Training Stage 1: Informal Reasoning\n",
            "============================================================\n",
            "\n",
            "Stage 1:   0% 0/2 [03:07<?, ?it/s]\n",
            "\n",
            "\n",
            "Error during training: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 9584 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 108.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/chained-adversarial-reasoning/run_unified_training.py\", line 129, in main\n",
            "    results = trainer.train_full_pipeline(problems)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/unified_trainer.py\", line 305, in train_full_pipeline\n",
            "    stage_metrics = self.train_stage(stage_id, problems, self.config.n_generator_steps)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/unified_trainer.py\", line 83, in train_stage\n",
            "    metrics = self._train_discriminator_step(problem, stage_id)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/unified_trainer.py\", line 168, in _train_discriminator_step\n",
            "    old_log_probs = self.model.get_log_probs(prompt, stage_tests)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/models/unified_model.py\", line 224, in get_log_probs\n",
            "    outputs = self.model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 449, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 1072, in wrapper\n",
            "    outputs = func(self, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 384, in forward\n",
            "    hidden_states = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
            "    return super().__call__(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 249, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 46, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/activations.py\", line 99, in forward\n",
            "    return nn.functional.silu(input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 2371, in silu\n",
            "    return torch._C._nn.silu(input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 9584 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 108.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/chained-adversarial-reasoning/run_unified_training.py\", line 151, in <module>\n",
            "    main()\n",
            "  File \"/content/chained-adversarial-reasoning/run_unified_training.py\", line 129, in main\n",
            "    results = trainer.train_full_pipeline(problems)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/unified_trainer.py\", line 305, in train_full_pipeline\n",
            "    stage_metrics = self.train_stage(stage_id, problems, self.config.n_generator_steps)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/unified_trainer.py\", line 83, in train_stage\n",
            "    metrics = self._train_discriminator_step(problem, stage_id)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/unified_trainer.py\", line 168, in _train_discriminator_step\n",
            "    old_log_probs = self.model.get_log_probs(prompt, stage_tests)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/models/unified_model.py\", line 224, in get_log_probs\n",
            "    outputs = self.model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 449, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 1072, in wrapper\n",
            "    outputs = func(self, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 384, in forward\n",
            "    hidden_states = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
            "    return super().__call__(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 249, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 46, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/activations.py\", line 99, in forward\n",
            "    return nn.functional.silu(input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 2371, in silu\n",
            "    return torch._C._nn.silu(input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 9584 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 108.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_training.py \\\n",
        "  --generator-model Qwen/Qwen2.5-Coder-0.5B-Instruct \\\n",
        "  --discriminator-model Qwen/Qwen2.5-Coder-0.5B-Instruct \\\n",
        "  --device cuda \\\n",
        "  --problems-file data/example_problems.json \\\n",
        "  --output-dir output \\\n",
        "  --checkpoint-dir checkpoints\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGlZyEwaW6Q0",
        "outputId": "65fa429a-78ff-4ec9-fcdc-0a1a5f3c54f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ADVERSARIAL RL MULTI-STAGE REASONING SYSTEM\n",
            "============================================================\n",
            "\n",
            "Configuration:\n",
            "  Generator Model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
            "  Discriminator Model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
            "  Device: cuda\n",
            "  N Discriminator Steps: 2\n",
            "  N Generator Steps: 2\n",
            "  K Alternating Steps: 2\n",
            "  Learning Rate: 1e-05\n",
            "\n",
            "Loading problems from data/example_problems.json...\n",
            "Loaded 5 problems\n",
            "\n",
            "Initializing models...\n",
            "Loading generator model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
            "tokenizer_config.json: 7.30kB [00:00, 21.8MB/s]\n",
            "vocab.json: 2.78MB [00:00, 122MB/s]\n",
            "merges.txt: 1.67MB [00:00, 146MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 186MB/s]\n",
            "config.json: 100% 659/659 [00:00<00:00, 5.39MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-12-01 01:30:42.059035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764552642.077556    1487 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764552642.082494    1487 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764552642.095672    1487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764552642.095702    1487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764552642.095706    1487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764552642.095710    1487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-01 01:30:42.099699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 988M/988M [00:16<00:00, 61.2MB/s]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.54MB/s]\n",
            "\n",
            "Loading discriminator model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
            "\n",
            "Creating adversarial trainer...\n",
            "\n",
            "Starting training...\n",
            "\n",
            "\n",
            "============================================================\n",
            "STARTING FULL PIPELINE TRAINING\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "Training Stage 1: Informal Reasoning\n",
            "============================================================\n",
            "\n",
            "Training discriminator at stage 1 for 2 steps...\n",
            "Discriminator:  50% 1/2 [02:51<02:51, 171.77s/it]\n",
            "\n",
            "Error during training: CUDA out of memory. Tried to allocate 402.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 322.12 MiB is free. Process 14244 has 14.42 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 193.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/chained-adversarial-reasoning/run_training.py\", line 171, in main\n",
            "    results = trainer.train_full_pipeline(problems)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/adversarial_trainer.py\", line 543, in train_full_pipeline\n",
            "    stage_metrics = self.train_stage(stage_id, problems)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/adversarial_trainer.py\", line 462, in train_stage\n",
            "    disc_metrics = self.train_discriminator_epoch(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/adversarial_trainer.py\", line 145, in train_discriminator_epoch\n",
            "    metrics = train_step(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/training/rl_loop.py\", line 80, in train_step\n",
            "    log_probs = model.get_log_probs(prompt, output)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/chained-adversarial-reasoning/models/discriminator.py\", line 159, in get_log_probs\n",
            "    outputs = self.model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 463, in forward\n",
            "    logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 402.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 322.12 MiB is free. Process 14244 has 14.42 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 193.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    }
  ]
}