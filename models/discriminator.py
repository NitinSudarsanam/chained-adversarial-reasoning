"""Discriminator LLM for generating adversarial tests and critiques."""

import torch
import re
from typing import List
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model


class LLMDiscriminator:
    """Discriminator model that generates adversarial tests and critiques."""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        """Initialize discriminator from HuggingFace model.
        
        Args:
            model_name: HuggingFace model identifier
            device: Device to run on ('cpu' or 'cuda')
        """
        self.model_name = model_name
        self.device = device
        
        print(f"Loading discriminator model: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        # Always use float32 for numerical stability (float16 can cause inf/nan issues)
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=device,
            quantization_config=quant_config
        )
        

        peft_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=[
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
            ],
        )

        base_model = prepare_model_for_kbit_training(model)
        self.model = get_peft_model(base_model, peft_config)

        for name, param in self.model.named_parameters():
            if "lora_" in name:
                param.requires_grad = True
            else:
                param.requires_grad = False

        self.model.eval()
        
        # Set pad token if not set
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def train(self):
        """Set model to training mode."""
        self.model.train()
        for name, param in self.model.named_parameters():
            if "lora_" in name:
                param.requires_grad = True
    
    def eval(self):
        """Set model to evaluation mode."""
        for name, param in self.model.named_parameters():
            if "lora_" in name:
                param.requires_grad = False
        self.model.eval()
    
    def parameters(self):
        """Return model parameters for optimizer."""
        # return self.model.parameters()
        return [param for name, param in self.model.named_parameters() if "lora_" in name]
    
    def generate_tests(
        self,
        problem: str,
        generator_code: str,
        num_tests: int = 5,
        prompt_template: str = None,
        max_new_tokens: int = 512,
        temperature: float = 0.8,
        top_p: float = 0.9
    ) -> str:
        """Generate adversarial test cases as a Python list of tuples.
        
        Args:
            problem: Problem description
            generator_code: Code generated by generator
            num_tests: Number of test cases to generate
            prompt_template: Optional custom prompt template
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature (higher = more creative)
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated test cases as a Python list of tuples: [(inputs, expected), ...]
        """
        if prompt_template is None:
            # Default template - should not normally be used (stages provide their own)
            prompt_template = """Generate test cases as a Python list of 2-tuples. Output ONLY the list.

Problem: {problem}

Code to test:
{stage_output}

Generate {num_tests} test cases in this format: [((arg1, arg2, ...), expected), ((arg1, arg2, ...), expected), ...]

CRITICAL: Each element MUST be a 2-tuple where:
- First element is a TUPLE of input arguments (even for single argument)
- Second element is the expected output value

Example: (([1, 2, 3],), 6)  <- note the comma after [1,2,3] to make it a tuple

Use canonical encodings for data structures:
- Linked lists as Python lists of values, e.g., [1, 2, 3] means 1->2->3
- Binary trees as level-order lists with None for missing children, e.g., [1, 2, 3, None, 4]

DO NOT write pytest functions. DO NOT write solution code. DO NOT write comments. ONLY the list.

```python
"""
        
        prompt = prompt_template.format(
            problem=problem,
            stage_output=generator_code,
            num_tests=num_tests
        )
        
        # Debug: Print first 500 chars of prompt to verify correctness
        print(f"\n[DEBUG] Discriminator prompt (first 500 chars):")
        print(prompt[:500])
        print("...")
        
        # Generate
        output = self._generate(prompt, max_new_tokens, temperature, top_p)
        
        # Debug: Show what was generated
        print(f"\n[DEBUG] Raw discriminator output (first 300 chars):")
        print(output[:300])
        
        # Validate output - if it looks like solution code, reject it
        if self._looks_like_solution_code(output):
            print(f"\n⚠️  WARNING: Discriminator generated solution code instead of tests!")
            print(f"   Returning empty string to skip this example.")
            return ""
        
        # Sanitize and extract code
        output = self._extract_code_from_markdown(output)
        output = self._sanitize_test_code(output)
        
        return output
    
    def _looks_like_solution_code(self, output: str) -> bool:
        """Check if output looks like solution code instead of test cases.
        
        Args:
            output: Generated output
            
        Returns:
            True if it looks like solution code
        """
        # Check for function definitions (should not be present in test lists)
        if 'def ' in output and '[' not in output[:100]:  # Function before any list
            return True
        
        # Check for class definitions
        if 'class ' in output:
            return True
        
        # Check for common solution patterns
        solution_keywords = ['while True:', 'for i in range', 'return result', 'if __name__']
        if any(keyword in output for keyword in solution_keywords) and '[' not in output[:200]:
            return True
        
        return False
    
    def generate_critique(
        self,
        problem: str,
        stage_output: str,
        stage_id: int,
        prompt_template: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """Generate critique for a reasoning stage.
        
        Args:
            problem: Problem description
            stage_output: Output from generator for this stage
            stage_id: Stage ID (1-5)
            prompt_template: Template for critique generation
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated critique
        """
        prompt = prompt_template.format(
            problem=problem,
            stage_output=stage_output
        )
        
        # Generate
        output = self._generate(prompt, max_new_tokens, temperature, top_p)
        
        # Sanitize
        output = output.strip()
        
        return output
    
    def get_log_probs(self, prompt: str, output: str) -> torch.Tensor:
        """Get log probabilities for RL training.
        
        Args:
            prompt: Input prompt
            output: Generated output
            
        Returns:
            Log probabilities tensor
        """
        # Handle empty output
        if not output or not output.strip():
            return torch.tensor([0.0], device=self.device, requires_grad=True)
        
        # Tokenize with reasonable max length
        full_text = prompt + output
        inputs = self.tokenizer(full_text, return_tensors="pt", truncation=True, max_length=4096).to(self.device)
        prompt_inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096).to(self.device)
        
        # Get model outputs (WITH gradients for training)
        outputs = self.model(**inputs)
        logits = outputs.logits
        
        # Get log probs for generated tokens only
        prompt_len = prompt_inputs.input_ids.shape[1]
        input_len = inputs.input_ids.shape[1]
        
        # Handle edge case where output is too short
        if input_len <= prompt_len:
            del inputs, logits, prompt_inputs
            return torch.tensor([0.0], device=self.device, requires_grad=True)
        
        generated_logits = logits[0, prompt_len-1:-1, :]
        generated_tokens = inputs.input_ids[0, prompt_len:]
        
        # Delete inputs now that we've extracted what we need
        del inputs
        
        # Handle empty generation
        if generated_tokens.shape[0] == 0:
            del logits, generated_logits, generated_tokens, prompt_inputs
            return torch.tensor([0.0], device=self.device, requires_grad=True)
        
        # Compute log probabilities
        log_probs = torch.nn.functional.log_softmax(generated_logits, dim=-1)
        token_log_probs = log_probs.gather(1, generated_tokens.unsqueeze(1)).squeeze(1)
        
        # Clean up intermediate tensors
        del logits, generated_logits, generated_tokens, log_probs, prompt_inputs
        
        return token_log_probs
    
    def _generate(
        self,
        prompt: str,
        max_new_tokens: int,
        temperature: float,
        top_p: float
    ) -> str:
        """Internal generation method.
        
        Args:
            prompt: Input prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated text
        """
        # CRITICAL: Always set to eval mode before generation
        was_training = self.model.training
        self.model.eval()
        
        # Format prompt using chat template for instruction-tuned models
        if hasattr(self.tokenizer, 'apply_chat_template') and self.tokenizer.chat_template:
            system_prompt = """You are a TEST CASE GENERATOR.

Your ONLY job is to generate valid test cases in a strict format.

CRITICAL INSTRUCTIONS:

1. You will receive:
   - A problem description
   - A function signature
   - (Optionally) reference code

2. You MUST generate test cases as a Python list of 2-tuples:
   [
       (input_args_tuple, expected_output),
       (input_args_tuple, expected_output),
       ...
   ]

3. "input_args_tuple" MUST be a tuple, even if the function takes only one argument.
   Example: ([ [1,0],[0,1] ],)  <- note the trailing comma

4. Each test case MUST contain EXACTLY two elements:
   - The tuple of input arguments
   - The expected output value

5. NEVER emit a flat list where only the last element is the expected output (e.g., [a, b, c, expected]). Every list element MUST be a 2-tuple of (inputs_tuple, expected_output).

7. All test cases MUST be valid Python.
   No missing parentheses, no trailing elements, no malformed lists, and no illegal board shapes.

8. DO NOT:
   - Write solution code
   - Write imports
   - Write comments
   - Write explanations
   - Output anything except the list inside ```python ``` markers

9. Your entire response MUST be:

```python
[
    ((...), expected_output),
    ((...), expected_output)
]
```"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
        else:
            formatted_prompt = prompt
        
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=4096).to(self.device)
        
        # Force the model to start with '[' by adding it to the prompt
        # This ensures it generates a list, not solution code
        bracket_token = self.tokenizer.encode('[', add_special_tokens=False)
        
        # Clamp temperature to safe range to avoid numerical issues
        temperature = max(0.1, min(2.0, temperature))
        top_p = max(0.1, min(1.0, top_p))
        
        with torch.no_grad():
            try:
                # Add opening bracket to force list format
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3,
                    # Force first token to be '['
                    forced_bos_token_id=bracket_token[0] if bracket_token else None
                )
            except Exception as e:
                # If generation fails, return empty string to skip this example
                print(f"Warning: Generation failed with error: {type(e).__name__}: {str(e)[:100]}")
                # Restore training mode if needed
                if was_training:
                    self.model.train()
                # Don't try to use CUDA operations after CUDA error - just return empty
                return ""
        
        # Decode only the generated part
        generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # Restore training mode if it was on before
        if was_training:
            self.model.train()
        
        return generated_text
    
    def _extract_code_from_markdown(self, text: str) -> str:
        """Extract code from markdown code blocks.
        
        Args:
            text: Text potentially containing markdown code blocks
            
        Returns:
            Extracted code or original text
        """
        # Look for ```python ... ``` blocks
        pattern = r'```python\s*(.*?)\s*```'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if matches:
            return matches[0].strip()
        
        # Look for ``` ... ``` blocks
        pattern = r'```\s*(.*?)\s*```'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if matches:
            return matches[0].strip()
        
        return text.strip()
    
    def _fix_common_syntax_errors(self, code: str) -> str:
        """Fix common syntax errors in generated test lists.
        
        The discriminator generates simple Python lists like:
        [([1, 2], 3), ([4, 5], 9)]
        
        This only needs minimal cleanup.
        
        Args:
            code: Code with potential syntax errors
            
        Returns:
            Code with common errors fixed
        """
        import re
        
        # Fix: Remove extra spaces inside brackets
        code = re.sub(r'\[\s+', '[', code)
        code = re.sub(r'\s+\]', ']', code)
        
        # Fix: Ensure space after commas (for readability)
        code = re.sub(r',(\S)', r', \1', code)
        
        return code
    
    def _sanitize_test_code(self, code: str) -> str:
        """Sanitize generated test code.
        
        Args:
            code: Raw generated test code (should be a Python list of tuples)
            
        Returns:
            Sanitized test code
        """
        # Find the first opening bracket - that's where the list starts
        lines = code.split('\n')
        first_bracket_idx = -1
        for i, line in enumerate(lines):
            if '[' in line:
                first_bracket_idx = i
                break
        
        if first_bracket_idx >= 0:
            # Start from the line with the bracket
            lines = lines[first_bracket_idx:]
            code = '\n'.join(lines)
        
        # Find the last closing bracket - that's where the list ends
        last_bracket_idx = -1
        for i in range(len(code) - 1, -1, -1):
            if code[i] == ']':
                last_bracket_idx = i
                break
        
        if last_bracket_idx >= 0:
            # Trim everything after the closing bracket
            code = code[:last_bracket_idx + 1]
        
        # Fix common syntax errors
        code = self._fix_common_syntax_errors(code)

        # Try to extract the first well-formed list and validate
        cleaned = self._extract_first_list(code)
        if not cleaned:
            return ""

        validated = self._validate_list_syntax(cleaned)
        return validated

    def _extract_first_list(self, text: str) -> str:
        """Extract the first bracket-balanced list substring."""
        start = text.find('[')
        if start == -1:
            return ""
        depth = 0
        for idx in range(start, len(text)):
            ch = text[idx]
            if ch == '[':
                depth += 1
            elif ch == ']':
                depth -= 1
                if depth == 0:
                    return text[start:idx+1].strip()
        return ""

    def _validate_list_syntax(self, text: str) -> str:
        """Return text if it literal-evals to a list, else empty string."""
        import ast
        # Strip python fences if present
        if text.startswith("```"):
            text = text.strip('`')
        try:
            obj = ast.literal_eval(text)
        except Exception:
            return ""
        if not isinstance(obj, list):
            return ""
        # Ensure elements look tuple/list-like with at least 2 items
        for item in obj:
            if not isinstance(item, (list, tuple)):
                return ""
            if len(item) < 2:
                return ""
        return text.strip()
