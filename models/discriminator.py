"""Discriminator LLM for generating adversarial tests and critiques."""

import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer


class LLMDiscriminator:
    """Discriminator model that generates adversarial tests and critiques."""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        """Initialize discriminator from HuggingFace model.
        
        Args:
            model_name: HuggingFace model identifier
            device: Device to run on ('cpu' or 'cuda')
        """
        self.model_name = model_name
        self.device = device
        
        print(f"Loading discriminator model: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32 if device == "cpu" else torch.float16,
            device_map=device
        )
        self.model.eval()
        
        # Set pad token if not set
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def train(self):
        """Set model to training mode."""
        self.model.train()
    
    def eval(self):
        """Set model to evaluation mode."""
        self.model.eval()
    
    def parameters(self):
        """Return model parameters for optimizer."""
        return self.model.parameters()
    
    def generate_tests(
        self,
        problem: str,
        generator_code: str,
        num_tests: int = 5,
        prompt_template: str = None,
        max_new_tokens: int = 512,
        temperature: float = 0.8,
        top_p: float = 0.9
    ) -> str:
        """Generate adversarial test cases as Python code.
        
        Args:
            problem: Problem description
            generator_code: Code generated by generator
            num_tests: Number of test cases to generate
            prompt_template: Optional custom prompt template
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature (higher = more creative)
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated test cases as pytest functions
        """
        if prompt_template is None:
            prompt_template = """You are generating test cases for a coding problem and solution.

Problem: {problem}

Generated Code:
{stage_output}

Generate {num_tests} challenging test cases as pytest functions. Make them adversarial - try to find edge cases and potential bugs.

Test Cases:
```python
import pytest

"""
        
        prompt = prompt_template.format(
            problem=problem,
            stage_output=generator_code,
            num_tests=num_tests
        )
        
        # Generate
        output = self._generate(prompt, max_new_tokens, temperature, top_p)
        
        # Sanitize and extract code
        output = self._extract_code_from_markdown(output)
        output = self._sanitize_test_code(output)
        
        return output
    
    def generate_critique(
        self,
        problem: str,
        stage_output: str,
        stage_id: int,
        prompt_template: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """Generate critique for a reasoning stage.
        
        Args:
            problem: Problem description
            stage_output: Output from generator for this stage
            stage_id: Stage ID (1-5)
            prompt_template: Template for critique generation
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated critique
        """
        prompt = prompt_template.format(
            problem=problem,
            stage_output=stage_output
        )
        
        # Generate
        output = self._generate(prompt, max_new_tokens, temperature, top_p)
        
        # Sanitize
        output = output.strip()
        
        return output
    
    def get_log_probs(self, prompt: str, output: str) -> torch.Tensor:
        """Get log probabilities for RL training.
        
        Args:
            prompt: Input prompt
            output: Generated output
            
        Returns:
            Log probabilities tensor
        """
        # Handle empty output
        if not output or not output.strip():
            return torch.tensor([0.0], device=self.device, requires_grad=True)
        
        # Tokenize
        full_text = prompt + output
        inputs = self.tokenizer(full_text, return_tensors="pt", truncation=True, max_length=2048).to(self.device)
        prompt_inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(self.device)
        
        # Get model outputs (WITH gradients for training)
        outputs = self.model(**inputs)
        logits = outputs.logits
        
        # Get log probs for generated tokens only
        prompt_len = prompt_inputs.input_ids.shape[1]
        
        # Handle edge case where output is too short
        if inputs.input_ids.shape[1] <= prompt_len:
            return torch.tensor([0.0], device=self.device, requires_grad=True)
        
        generated_logits = logits[0, prompt_len-1:-1, :]
        generated_tokens = inputs.input_ids[0, prompt_len:]
        
        # Handle empty generation
        if generated_tokens.shape[0] == 0:
            return torch.tensor([0.0], device=self.device, requires_grad=True)
        
        # Compute log probabilities
        log_probs = torch.nn.functional.log_softmax(generated_logits, dim=-1)
        token_log_probs = log_probs.gather(1, generated_tokens.unsqueeze(1)).squeeze(1)
        
        return token_log_probs
    
    def _generate(
        self,
        prompt: str,
        max_new_tokens: int,
        temperature: float,
        top_p: float
    ) -> str:
        """Internal generation method.
        
        Args:
            prompt: Input prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated text
        """
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(self.device)
        
        # Clamp temperature to safe range to avoid numerical issues
        temperature = max(0.1, min(2.0, temperature))
        top_p = max(0.1, min(1.0, top_p))
        
        with torch.no_grad():
            try:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,  # Prevent repetition issues
                    no_repeat_ngram_size=3   # Prevent exact repetitions
                )
            except (RuntimeError, torch.cuda.CudaError) as e:
                # If generation fails due to CUDA error, try with greedy decoding
                print(f"Warning: Generation failed with sampling, falling back to greedy: {e}")
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,  # Greedy decoding
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
        
        # Decode only the generated part
        generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return generated_text
    
    def _extract_code_from_markdown(self, text: str) -> str:
        """Extract code from markdown code blocks.
        
        Args:
            text: Text potentially containing markdown code blocks
            
        Returns:
            Extracted code or original text
        """
        # Look for ```python ... ``` blocks
        pattern = r'```python\s*(.*?)\s*```'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if matches:
            return matches[0].strip()
        
        # Look for ``` ... ``` blocks
        pattern = r'```\s*(.*?)\s*```'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if matches:
            return matches[0].strip()
        
        return text.strip()
    
    def _sanitize_test_code(self, code: str) -> str:
        """Sanitize generated test code.
        
        Args:
            code: Raw generated test code
            
        Returns:
            Sanitized test code
        """
        # Ensure pytest import
        if 'import pytest' not in code:
            code = 'import pytest\n\n' + code
        
        # Remove incomplete test functions at the end
        lines = code.split('\n')
        
        # Find last complete test function
        last_complete = len(lines)
        in_function = False
        function_indent = 0
        
        for i, line in enumerate(lines):
            if line.strip().startswith('def test_'):
                in_function = True
                function_indent = len(line) - len(line.lstrip())
            elif in_function and line.strip() and not line.startswith(' ' * (function_indent + 1)):
                # Function ended
                in_function = False
                last_complete = i
        
        # If we're still in a function at the end, check if it looks complete
        if in_function:
            # Look for at least one assert or pass statement
            function_lines = lines[-(len(lines) - last_complete):]
            has_content = any('assert' in line or 'pass' in line for line in function_lines)
            if not has_content:
                lines = lines[:last_complete]
        
        code = '\n'.join(lines)
        
        return code.strip()
